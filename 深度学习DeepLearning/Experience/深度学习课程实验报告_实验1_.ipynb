{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img alt=\"jupyter\" height=\"1080\" src=\"./output.png\" width=\"1420\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 基本信息\n",
    "1. 实验名称：实验一 pytorch基本操作与logistics、softmax回归的实现\n",
    "2. 姓名：\n",
    "3. 学号：\n",
    "4. 日期：2022.10.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 一、任务1 Pytorch基本操作考察\n",
    "\n",
    "## 1.1 任务内容\n",
    "\n",
    "见实验要求\n",
    "\n",
    "## 1.2 任务思路及代码\n",
    "简单实现即可"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M tensor([[0.1937, 0.8200, 0.7700]])\n",
      "N tensor([[0.5907],\n",
      "        [0.0518]])\n",
      "M - N tensor([[-0.3970,  0.2292,  0.1793],\n",
      "        [ 0.1420,  0.7682,  0.7183]])\n",
      "M.subtract(N) tensor([[-0.3970,  0.2292,  0.1793],\n",
      "        [ 0.1420,  0.7682,  0.7183]])\n",
      "torch.subtract(M, N) tensor([[-0.3970,  0.2292,  0.1793],\n",
      "        [ 0.1420,  0.7682,  0.7183]])\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand(1, 3)\n",
    "N = torch.rand(2, 1)\n",
    "print(\"M\", M)\n",
    "print(\"N\", N)\n",
    "print(\"M - N\", M - N)\n",
    "print(\"M.subtract(N)\", M.subtract(N))\n",
    "print(\"torch.subtract(M, N)\", torch.subtract(M, N))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 实验结果分析\n",
    "第一种 调用tensor类的运算符重载直接计算\n",
    "第二种 是类函数\n",
    "第三种是torch的外部函数\n",
    "运算过程中出现广播，最后都变成了 shape 2*3 矩阵减法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P tensor([[ 0.0067,  0.0059],\n",
      "        [-0.0033,  0.0239],\n",
      "        [-0.0104, -0.0007]])\n",
      "Q tensor([[-0.0091,  0.0119],\n",
      "        [-0.0137,  0.0097],\n",
      "        [ 0.0125,  0.0039],\n",
      "        [-0.0045, -0.0084]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.normal(0, 0.01, size=(3, 2))\n",
    "Q = torch.normal(0, 0.01, size=(4, 2))\n",
    "print(\"P\",P)\n",
    "print('Q',Q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the inverse of Q\n",
      "tensor([[-0.0091, -0.0137,  0.0125, -0.0045],\n",
      "        [ 0.0119,  0.0097,  0.0039, -0.0084]])\n"
     ]
    }
   ],
   "source": [
    "print(\"the inverse of Q\")\n",
    "print(Q.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P dot Q\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 9.1661e-06, -3.4657e-05,  1.0619e-04, -7.9158e-05],\n        [ 3.1417e-04,  2.7680e-04,  5.2596e-05, -1.8504e-04],\n        [ 8.5815e-05,  1.3545e-04, -1.3219e-04,  5.2750e-05]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"P dot Q\")\n",
    "P@Q.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y_1 = x * x\n",
    "y_2 = (x*x*x).detach()\n",
    "y_3 = y_2 + y_1\n",
    "y_3.backward()\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在 x^3 使用detach 相当于 对于 y3来说 y2是个常量 不求其导数，最后只有y2在反向传播计算图中被计算了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 二、任务2 实现logistics回归\n",
    "## 2.1 从零开始实现logistics回归"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "# 从零开始实现logistics回归\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def mylogistics(X):\n",
    "    z =  X.matmul(w.T) + b\n",
    "    return torch.sigmoid(z)\n",
    "    # result = torch.where(f>0.5, 1.0, 0.0)\n",
    "\n",
    "\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def mySGD(params, lr):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr*param.grad\n",
    "            param.grad.zero_()\n",
    "\n",
    "x = torch.randint(-100,100, size=(100, 2), dtype=torch.float32)\n",
    "label = []\n",
    "for i in range(x.shape[0]):\n",
    "    if x[i, 0] < x[i, 1]:\n",
    "        label.append(1.0)\n",
    "    else:\n",
    "        label.append(0.0)\n",
    "\n",
    "y = torch.tensor(label)\n",
    "y = y.reshape((-1, 1))\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def acc_logistics(y_hat, y):\n",
    "    result = torch.where(y_hat>=0.5, 1, 0)\n",
    "    return (result == y).type(y_hat.dtype).float().sum() / len(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "y_hat_tem = torch.tensor([0.3, 0.9])\n",
    "y_tem = torch.tensor([1, 0])\n",
    "print(acc_logistics(y_hat_tem, y_tem).item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def train_logistics_with_nothing(epochs, net, loss, lr, x, y, w, b):\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = net(x)\n",
    "        # print(y_hat)\n",
    "        # print(y_hat.shape)\n",
    "        acc = acc_logistics(y_hat, y)\n",
    "        #  need a ()\n",
    "        l = loss()(y_hat, y)\n",
    "        l.backward()\n",
    "        mySGD([w, b], lr)\n",
    "        if ((epoch+1) % 100==0):\n",
    "            print(\"epoch\", epoch, \" loss is\", l.item(), \" acc is\", acc.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([100, 1])\n",
      "epoch 99  loss is 0.07730719447135925  acc is 1.0\n",
      "epoch 199  loss is 0.05772613361477852  acc is 1.0\n",
      "epoch 299  loss is 0.047977838665246964  acc is 1.0\n",
      "epoch 399  loss is 0.04174986481666565  acc is 1.0\n",
      "epoch 499  loss is 0.037301160395145416  acc is 1.0\n",
      "epoch 599  loss is 0.033910952508449554  acc is 1.0\n",
      "epoch 699  loss is 0.031214550137519836  acc is 1.0\n",
      "epoch 799  loss is 0.029003391042351723  acc is 1.0\n",
      "epoch 899  loss is 0.02714785747230053  acc is 1.0\n",
      "epoch 999  loss is 0.025562390685081482  acc is 1.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(-100,100, size=(100, 2), dtype=torch.float32)\n",
    "label = []\n",
    "for i in range(x.shape[0]):\n",
    "    if x[i, 0] < x[i, 1]:\n",
    "        label.append(1.0)\n",
    "    else:\n",
    "        label.append(0.0)\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(1, x.shape[1]), dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "y = torch.tensor(label)\n",
    "y = y.reshape((-1, 1))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "net = mylogistics\n",
    "\n",
    "train_logistics_with_nothing(1000, mylogistics, torch.nn.BCELoss, lr, x, y, w, b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 利用torch.nn完成logistics回归"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# 利用torch.nn完成logistics回归\n",
    "# 这里利用torch.nn.Module 的继承写了个新类，其实只要有个nn.Linear层加个sigmod就好。甚至好像BCEwithLogistics函数自带一个sigmode层。\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 函数\n",
    "\n",
    "class myLogisticsWithNN(nn.Module):\n",
    "    def __inti__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "            z =  X.matmul(w.T) + b\n",
    "            return torch.sigmoid(z)\n",
    "\n",
    "def acc_logistics(y_hat, y):\n",
    "    result = torch.where(y_hat>=0.5, 1, 0)\n",
    "    return (result == y).type(y_hat.dtype).float().sum() / len(y)\n",
    "\n",
    "\n",
    "def mySGD(params, lr):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr*param.grad\n",
    "            param.grad.zero_()\n",
    "\n",
    "\n",
    "def train_logistics_with_nn(epochs, net, loss, lr, x, y, w, b):\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = net().forward(x)\n",
    "        # print(y_hat)\n",
    "        # print(y_hat.shape)\n",
    "        acc = acc_logistics(y_hat, y)\n",
    "        #  need a ()\n",
    "        l = loss()(y_hat, y)\n",
    "        l.backward()\n",
    "        mySGD([w, b], lr)\n",
    "        if ((epoch+1) % 100==0):\n",
    "            print(\"epoch\", epoch, \" loss is\", l.item(), \" acc is\", acc.item())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([100, 1])\n",
      "epoch 99  loss is 0.053681716322898865  acc is 1.0\n",
      "epoch 199  loss is 0.03993070498108864  acc is 1.0\n",
      "epoch 299  loss is 0.0333995446562767  acc is 1.0\n",
      "epoch 399  loss is 0.029321296140551567  acc is 1.0\n",
      "epoch 499  loss is 0.026438962668180466  acc is 1.0\n",
      "epoch 599  loss is 0.02425103262066841  acc is 1.0\n",
      "epoch 699  loss is 0.022510986775159836  acc is 1.0\n",
      "epoch 799  loss is 0.02108093537390232  acc is 1.0\n",
      "epoch 899  loss is 0.01987663470208645  acc is 1.0\n",
      "epoch 999  loss is 0.018843140453100204  acc is 1.0\n"
     ]
    }
   ],
   "source": [
    "# 数据随件初始化和训练\n",
    "\n",
    "x = torch.randint(-100,100, size=(100, 2), dtype=torch.float32)\n",
    "label = []\n",
    "for i in range(x.shape[0]):\n",
    "    if x[i, 0] < x[i, 1]:\n",
    "        label.append(1.0)\n",
    "    else:\n",
    "        label.append(0.0)\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(1, x.shape[1]), dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "y = torch.tensor(label)\n",
    "y = y.reshape((-1, 1))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "net = mylogistics\n",
    "\n",
    "train_logistics_with_nn(epochs, myLogisticsWithNN, nn.BCELoss, lr, x, y, w, b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 三、任务3 实现softmax\n",
    "## 3.1 从零开始实现softmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 从零开始实现softmax\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "minst_train = torchvision.datasets.FashionMNIST(root=\"./Datasets/FashionMNIST\", train=True, download=True, transform=transforms.ToTensor())\n",
    "minst_test = torchvision.datasets.FashionMNIST(root=\"./Datasets/FashionMNIST\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 256\n",
    "train_iter = DataLoader(minst_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(minst_test, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "num_inputs = 784 #image of fashion minst is 28*28\n",
    "num_outputs = 10 # and the num of class is 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    return X_exp / X_exp.sum(1, keepdim=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return (- torch.log(y_hat[range(len(y_hat)), y])).mean()\n",
    "# to gain a scalar\n",
    "# for-loop is slow. we use a slice of y_hat to gain the all right class' prob, then log them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    result = y_hat.type(y.dtype) == y\n",
    "    # op== is very sensitive with data type, so call tensor.type() to stay the same with y\n",
    "    return float(result.type(y.dtype).sum())/len(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def mysgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def eval_acc(net, data_iter):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            result.append(accuracy(net(X), y))\n",
    "    return np.array(result).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train loss 0.35626840591430664 train acc 0.875 test_acc 0.8451171875\n",
      "epoch 3 train loss 0.4013982117176056 train acc 0.8645833333333334 test_acc 0.844921875\n",
      "epoch 4 train loss 0.4367528259754181 train acc 0.8541666666666666 test_acc 0.84443359375\n",
      "epoch 5 train loss 0.36640170216560364 train acc 0.875 test_acc 0.84521484375\n",
      "epoch 6 train loss 0.49508559703826904 train acc 0.8125 test_acc 0.8453125\n",
      "epoch 7 train loss 0.3980194628238678 train acc 0.8958333333333334 test_acc 0.84443359375\n",
      "epoch 8 train loss 0.32481905817985535 train acc 0.9166666666666666 test_acc 0.84521484375\n",
      "epoch 9 train loss 0.4401278793811798 train acc 0.8333333333333334 test_acc 0.8443359375\n",
      "epoch 0 train loss 0.3712468147277832 train acc 0.8645833333333334 test_acc 0.8453125\n",
      "epoch 1 train loss 0.40290477871894836 train acc 0.875 test_acc 0.8453125\n",
      "epoch 2 train loss 0.33752119541168213 train acc 0.875 test_acc 0.84521484375\n",
      "epoch 3 train loss 0.45941439270973206 train acc 0.7916666666666666 test_acc 0.8451171875\n",
      "epoch 4 train loss 0.485528826713562 train acc 0.84375 test_acc 0.84521484375\n",
      "epoch 5 train loss 0.3609320819377899 train acc 0.8645833333333334 test_acc 0.8453125\n",
      "epoch 6 train loss 0.4143246114253998 train acc 0.8333333333333334 test_acc 0.84521484375\n",
      "epoch 7 train loss 0.4485393464565277 train acc 0.8541666666666666 test_acc 0.8451171875\n",
      "epoch 8 train loss 0.4441925585269928 train acc 0.8541666666666666 test_acc 0.8453125\n",
      "epoch 9 train loss 0.411615252494812 train acc 0.8229166666666666 test_acc 0.8451171875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def train_softmax(net, train_iter, loss, updater):\n",
    "updater = torch.optim.SGD([W, b], lr=0.001)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for X, y in train_iter:\n",
    "        # print(X.shape)\n",
    "        # print(X)\n",
    "        # print(y.shape)\n",
    "        y_hat = net(X)\n",
    "        # print(y_hat)\n",
    "        # print(y)\n",
    "        l = cross_entropy(y_hat, y)\n",
    "        # print(l)\n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        updater.step()\n",
    "    print(\"epoch\", epoch, \"train loss\", l.item(), \"train acc\", accuracy(y_hat, y), \"test_acc\", eval_acc(net, test_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 利用torch.nn实现 softmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "\n",
    "# 这里参考了 李沐学ai的 3.7. softmax回归的简洁实现 里面的trick，即logsumexp技巧，在交叉熵里变相完成了softmax，这样避免上溢和下溢；而不是像上边一样利用nn.module实现一个新的class softmax。byt，李沐书中的这个方法真的太强了。\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "minst_train = torchvision.datasets.FashionMNIST(root=\"./Datasets/FashionMNIST\", train=True, download=True, transform=transforms.ToTensor())\n",
    "minst_test = torchvision.datasets.FashionMNIST(root=\"./Datasets/FashionMNIST\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 256\n",
    "train_iter = DataLoader(minst_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(minst_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss1 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "net1 = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(784, 10))\n",
    "updater1 = torch.optim.SGD(net1.parameters(), lr=0.1)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net1.apply(init_weights)\n",
    "\n",
    "epochs = 10\n",
    "def train_softmax_with_nn(epoch):\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net1(X)\n",
    "            # print(y_hat)\n",
    "            l = loss1(y_hat, y)\n",
    "            updater1.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater1.step()\n",
    "        print(\"epoch\", epoch, \"train loss\", l.mean().item(), \"train acc\", accuracy(y_hat, y), \"test_acc\", eval_acc(net, test_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 0.6546421647071838 train acc 0.78125 test_acc 0.8451171875\n",
      "epoch 1 train loss 0.5154418349266052 train acc 0.8541666666666666 test_acc 0.8451171875\n",
      "epoch 2 train loss 0.552357017993927 train acc 0.8020833333333334 test_acc 0.8451171875\n",
      "epoch 3 train loss 0.5014792084693909 train acc 0.8229166666666666 test_acc 0.8451171875\n",
      "epoch 4 train loss 0.6346695423126221 train acc 0.8020833333333334 test_acc 0.8451171875\n",
      "epoch 5 train loss 0.5494449734687805 train acc 0.8020833333333334 test_acc 0.8451171875\n",
      "epoch 6 train loss 0.6015473008155823 train acc 0.7708333333333334 test_acc 0.8451171875\n",
      "epoch 7 train loss 0.4758111536502838 train acc 0.8333333333333334 test_acc 0.8451171875\n",
      "epoch 8 train loss 0.29127898812294006 train acc 0.90625 test_acc 0.8451171875\n",
      "epoch 9 train loss 0.41524577140808105 train acc 0.875 test_acc 0.8451171875\n"
     ]
    }
   ],
   "source": [
    "train_softmax_with_nn(epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A1 实验心得\n",
    "1. 在重名传递时，有时应该传递函数地址，有时又应该新建一个对象，直接传递，这里十分容易出错。比如在BCEloss中，一开始就是简单传递地址，结果爆出 不止一个可以比较的对象 之类的错误，由于经验不足简直无法入手。最后幸好，在同学的帮助下定位到了错误。\n",
    "2. 一开始，从零开始实现logistics回归用了BCEwithLogistics损失函数，发现loss一直不降，但是acc接近1.0。查询文档后知道BCELogistics自带一个sigmod函数，可以直接配合Linear函数使用。遂换成了BCEloss函数。\n",
    "3. softmax回归中通过参数类型转换很好的实现了acc函数。由bool转为float 直接记出正确的分类。\n",
    "# A2 参考文献\n",
    "> 动手学深度学习 https://zh-v2.d2l.ai/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}